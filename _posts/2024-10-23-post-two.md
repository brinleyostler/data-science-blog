---
layout: post
title:  "AZ's Most Popular Books: Web-Scraping in Action"
date: 2024-10-23
description: Using XPath in Selenium to scrape data from a Phoenix digital library database.   
image: "/assets/img/desert-books.png"
---
<p class="intro"><span class="dropcap">M</span>y favorite thing to do is write my stats blog. But when I'm not writing my blog, I like to read.</p>

#### Project Motivation

In this project, I set out to analyze a dataset of the top books (and audiobooks) from my local library. My goal was to uncover patterns with the most popular titles, authors, and formats. I wanted to answer questions like:
* Which books and authors are most popular?
* Are most books currently available or wait-list only?
* What is the average wait time for each book type?

To answer these questions, I used web-scraping techniques in Python, specifically leveraging Selenium and XPath, to gather this data from my hometown library’s digital database.


#### Quick note on webscraping practices:

Whenever we as data scientists want to scrape data from the web, we have to ensure we are following best practice. I checked the [site permissions](https://phoenix.overdrive.com/robots.txt) and found basically zero restrictions. So, I decided to include a time delay in my code to follow [best practice guidelines](https://research.aimultiple.com/web-scraping-best-practices/).


## Data Collection Process

Here's a summary of the main steps I followed for my data collection. For a full document of my code, check out my GitHub respository [best-selling-books](https://github.com/brinleyostler/best-selling-books).


### 1. Set up the Selenium environment

When using Selenium, you can use it with practically any sort of web driver. My default browser is Google Chrome, so I set up my environment with the Chrome WebDriver. You can look into more driver options [here](https://www.selenium.dev/documentation/webdriver/drivers/options/).

I chose to use Selenium so I could scrape data beyond the first page. Thus, I needed to use XPath to advance to the next screen by clicking the 'next page' button at the bottom of the page. There may be other ways to di this, but I found this method the most straightforward for this project.


### 2. Scraping the Main Websites

I ultimately wanted to scrape the entire dataset, but there were a lot of pages (over 42!), and it would have taken a much longer time to scrape, especially with the time delay I included in my code. So, I chose to only scrape the first 10 pages of both the top [ebooks](https://phoenix.overdrive.com/collection/25972?page=1&sortBy=mostpopular-site&maxItems=1000&mediaTypes=ebook) and the top [audioboks](https://phoenix.overdrive.com/collection/25978) in the database. This resulted in a total of the most popular 480 books (240 of each book type).

Using a while loop, I navigated through the first ten pages of the ebooks and audiobooks sections on the library’s site. I extracted the titles, authors, availability status, format, link, and rank for each item. 


### 3. Scraping the Individual Books' Sites

I wanted more information on each individual book, so I constructed another while loop to iterate through the links I had just collected. In this loop, I then gathered the number of copies, ratings, and wait times. 

This is the process that takes the longest. There are options you can employ (such as a headless browser) that allow it to operate faster. However, I wanted to monitor my code as it ran, so my code will reflect that decision.


### 4. Cleaning and Assembling the DataFrame

Once all the data had been collected, there were a few things I wanted to clean up and add to my dataset. I made sure all the types were appropriate for the dataset:
* I checked the types of each of my columns and made sure they were appropriate (type float or type int for my numeric columns and type str for my categorical columns).
* At this point, I created a Pandas dataframe with the Rank, Title, Author, Rating, Format, Copies, Availability, and Wait Time.

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Rank</th>
      <th>Title</th>
      <th>Author</th>
      <th>Rating</th>
      <th>Format</th>
      <th>Copies</th>
      <th>Availability</th>
      <th>Wait Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>The Women</td>
      <td>Kristin Hannah</td>
      <td>4.4</td>
      <td>EBOOK</td>
      <td>25.0</td>
      <td>WAIT LIST</td>
      <td>6 months</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Five Total Strangers</td>
      <td>Natalie D. Richards</td>
      <td>3.5</td>
      <td>EBOOK</td>
      <td>12.0</td>
      <td>AVAILABLE</td>
      <td>No wait</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>It Ends With Us</td>
      <td>Colleen Hoover</td>
      <td>4.3</td>
      <td>EBOOK</td>
      <td>15.0</td>
      <td>WAIT LIST</td>
      <td>6 months</td>
    </tr>
  </tbody>
</table>
</div>


* I then created a new column that calculated the number of weeks of wait time. This included converting 'No wait' entries to 0, and converting the 'months' entries to weeks by removing the text and multiplying them by 4. I made sure that these data points were of type float.


## Final Dataset

My final pandas DataFrame:
* Total entries: 480 (240 books & 240 audiobooks)
* Features: 9 

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Rank</th>
      <th>Title</th>
      <th>Author</th>
      <th>Rating</th>
      <th>Format</th>
      <th>Copies</th>
      <th>Availability</th>
      <th>Wait Time</th>
      <th>Wait Weeks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>The Women</td>
      <td>Kristin Hannah</td>
      <td>4.4</td>
      <td>EBOOK</td>
      <td>25.0</td>
      <td>WAIT LIST</td>
      <td>6 months</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Five Total Strangers</td>
      <td>Natalie D. Richards</td>
      <td>3.5</td>
      <td>EBOOK</td>
      <td>12.0</td>
      <td>AVAILABLE</td>
      <td>No wait</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>It Ends With Us</td>
      <td>Colleen Hoover</td>
      <td>4.3</td>
      <td>EBOOK</td>
      <td>15.0</td>
      <td>WAIT LIST</td>
      <td>6 months</td>
      <td>24.0</td>
    </tr>
  </tbody>
</table>
</div>


## Key Insights & Summary Statistics

In my next [post](https://brinleyostler.github.io/data-science-blog/blog/post-three/), I will go into more depth and detail with my exploratory data analysis. But, you can find some interesting takeaways from my webscraping dataset here:

#### Availability

At the time this post was written, 