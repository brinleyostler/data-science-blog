---
layout: post
title:  "AZ's Most Popular Books: Web-Scraping in Action"
date: 2024-10-23
description: Using XPath in Selenium to scrape data from a local website.   
image: "/assets/img/image5.jpg"
---
<p class="intro"><span class="dropcap">M</span>y favorite thing to do is write my stats blog. But when I'm not writing my blog, I like to read.</p>

#### Project Motivation
In this project, I set out to analyze a dataset of the top books (and audiobooks) from my local library. My goal was to uncover patterns with the most popular titles, authors, and formats. 
* Which books are most popular?
* Are most books currently available or wait-list only?
* Does a book's popularity (ranking) relate to the total number of copies?
* What is the average wait time for each book type?
* Which author has the most books on the popularity list?

To answer these questions, I used web-scraping techniques in Python, specifically leveraging Selenium and XPath, to gather this data from my hometown library’s digital database.

#### Quick note on webscraping practices:
Whenever we as data scientists want to scrape data from the web, we have to ensure we are following best practice. Check that site's robots.txt file (accessible at the base url/robots.txt). Good scraping practice ensures we are using delay functions in our, only scraping necessary data, and not accessing sensitive or restricted information.
Before starting, I verified that the library's terms of service allowed for this type of data collection, ensuring that I followed any restrictions on data usage.

## Data Collection Process
Here's a summary of the main steps I followed for my data collection. For a full document of my code, check out by GitHub [best-selling-books] (https://github.com/brinleyostler/best-selling-books) repository.

1. Set up the Selenium environment
When using Selenium, you can use it with practically any sort of web driver. My default is Google Chrome, so I set up my environment with the Chrome WebDriver. You can look into more driver options [here] (https://www.selenium.dev/documentation/webdriver/drivers/options/).

I chose to use Selenium so I could scrape data beyond the first page. Thus, I needed to use XPath to advance the page.

2. Scraping the Main Websites
I ultimately wanted to scrape the entire dataset, but there were a lot of pages, and it would have taken a much longer time to scrape, especially with the delay I included in my code. So, I chose to only scrape the first 10 pages of both the top [ebooks] (https://phoenix.overdrive.com/collection/25972?page=1&sortBy=mostpopular-site&maxItems=1000&mediaTypes=ebook) and the top [audioboks] (https://phoenix.overdrive.com/collection/25978). This resulted in the top 480 books (240 of each book type).

Using a while loop, I navigated through the first ten pages of the ebooks and audiobooks sections on the library’s site. I extracted the titles, authors, availability status, format, link, and rank for each item. 

3. Scraping the Individual Books' Sites
I wanted more information on each individual book, so I constructed another while loop to iterate through the links I had just collected. In this loop, I then gathered the number of copies, ratings, and wait times. 

This is the process that takes the longest. There are options you can employ to avoid opening the web driver in a window and allow it to operate faster. However, I wanted to monitor my code as it ran, so my code will reflect that.

4. Cleaning and Assembling the DataFrame
Once my data had all been collected, 
